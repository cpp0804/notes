
## 参考博文
[程序员面试时这样介绍自己的项目经验，成功率能达到99.99%](https://cloud.tencent.com/developer/news/631290)
[如何准备阿里P6/P7面试（项目经验篇）](https://toutiao.io/posts/e5brcg0/preview)

![框架](./pic/项目准备_框架.jpg)

# 1. 自我介绍
面试官你好，我的名字叫陈培培，浙江温州人，毕业于浙江大学软件学院。毕业后在阿里的营销平台工作了两年，在这两年里负责商家侧相关工作的Java开发。营销平台指的是

# 2. 统一投放系统
这个系统是为导购业务提供数据来源支持的。比如当我们打开一个页面，会看到很多的推荐商品。然后在不同时间、针对不同人群推荐的商品都不一样。统一投放系统解决的问题就是在什么时候、给什么人展示什么商品。

1. 在这个项目中，我第一个负责的就是action调度这一块。Action是为一系列已经写好的任务定义的调度模型。比如需要定时统计已经报名商家的数目。我把我的分解成2部分，一部分是模型设计：定义了一个ActionInstance作为任务实例, 每个ActionInstance就对应一个具体的任务，他定义了任务的code，要根据code路由到具体要被调度的任务，并且定义了任务执行时间，任务执行次数，任务执行间隔，任务失败次数，执行任务需要的参数。并且定义了ActionRecord记录任务的执行结果。
然后有个定时任务定时执行，遍历ActionInstance，如果当前时间大于任务执行时间，那么就执行任务，如果任务执行成功，那么他的下次执行时间设置为当前时间加上间隔时间。如果执行失败，看任务失败过多少次，如果失败次数大于3次，那就不再重试，否则重试。并且每次的任务执行结果都插入结果表中。

![action调度](./腾讯/pic/项目准备_action调度.png)

定时任务的话，他是在页面上创建一个任务。需要填入任务类型。比如java、c++，然后要填入任务的全限定类名，然后指定是单机跑还是集群跑。然后开发的话，要继承JavaProcessor，实现process方法。

执行任务的方式：
```
1. 单机：一个任务实例只会随机触发到一台机器上
2. 广播：会广播给所有机器执行，当所有机器都执行完成，该任务才算完成。任意一台机器执行失败，都算该任务失败
3. MapReduce任务：把一批子任务分布式到多台机器上执行。
```

```java
@Component
public class MyProcessor2 extends JavaProcessor {
    private volatile boolean stop = false;

    @Override
    public ProcessResult process(JobContext context) throws Exception {
        int N = 10000;
        while (!stop && N >= 0) {
            //TODO
            N--;
        }
        return new ProcessResult(true);
    }

    @Override
    public void kill(JobContext context) {
        stop = true;
    }

    @Override
    public void preProcess(JobContext context) {
        stop = false;  //如果是通过Spring启动，Bean是单例，需要通过preProcess把标记为复位
    }
} 
```

2. 我第二个接的任务是投放排期的复制。排期其实就是运营在投放侧设置的一份规则。在每次活动中，运营都要创建非常多的规则，但其实很多排期其实他们的差异不大，可能只有在时间上有差异，所以运营希望能再已有排期的基础上修改一小点地方，然后得到一个新的排期。所以我们采取的方案就是复制排期。在我写代码之前，我先去弄清楚啥是排期，然后总结一下我感觉排期其实可以分为三部分，一部分是时间，他定义了这份排期的生效时间，第二部分是数据，这个数据就是选品得出来的一个个池子，第三部分是排序规则，它定义了数据的排序顺序。所以其实我的目标就是搞定这三部分。我把任务分解成几个步骤。一个是根据id获取已有的排期，然后创建一个新排期，把数据复制到新排期中，最后保存这个新排期。我觉得这里面最复杂的是排期数据区的复制，排期其实有5种类型的数据源，因为信息安全我不能细说，所以我要搞清楚每种数据源的链路。具体我的做法是，定义了一个抽象数据处理器，然后5种数据源都是他的实现类，复制的时候根据排期具体的数据类型选择具体的数据源处理器。我觉得这样比较容易扩展，如果以后添加了数据类型的话，就添加一个实现类就行了，不需要做其他改动。
# 3. A社交网站
这是模仿微博的一个社交网站，注册成为网站的用户可以发布动态、关注好友、查看好友动态，对好友动态点赞、收藏

我们用的是Neo4j, 他是基于图的数据库，他的节点代表实体，边代表联系，然后边也可以
有属性。比如一个人发布了动态，那么发布这个边就可以有发布时间这个属性。

担任后端的组长，后端组3个人。搭建好框架后，让每个人自己做自己想做的模块，大致分配。
框架搭建问题。对neo4j数据库不熟悉，数据库的连接出了一些问题。后面就是官方文档看，csdn上别人怎么做的

自己定义的查询方法不能遍历到他关联的节点。比如说如果我自己写语句得到了一个user节点，本来我是可以通过get方法获取这个user关联的动态列表的，但是通过这个user节点我获取的动态列表是空的。刚开始一直找不出问题。后来才发现如果要通过user遍历他关联的节点的话，必须是使用neo4jRepository已经为我们提供的getUser()方法，通过这个方法获取到的对象就可以遍历他关联的节点。
查博客，问组内同学，问其他项目组的同学

Save方法继承了update和persist
可以遍历到所有关联的节点
NodeEntity
RepationshipEntity
startNode
endNode

## 面试问题
大V发了一条微博，如何让关注他的人马上看到:
用户发表微博后，把微博写入消息队列。


# 4. 教务管理系统
这个项目是我本科的时候学院举办的一个开发的比赛。我是一个人从系统的分析、设计、开发一直做下去的。差不多做了5个多月。
我的整个设计分为3个阶段。第一个阶段是画流程图，通过画流程图来分析这个系统需要包含什么模块，然后搞清楚每个业务的流程是什么样的，会产生什么数据。然后流程图是我后面设计的依据。
第二阶段就是系统设计，我主要画了类图，这个里面包含了实体类、service类和dao类。
第三阶段是进行数据库设计。这一阶段的设计是根据画好的类图转换成数据库表

## 问题
因为这是我做的第一个系统，遇到了不少的问题
1.	第一个是我之前没有做过系统，都不知道从哪里入手。对类的设计都很陌生，后面才从流程图入手，理清自己的思路。
2.	第二个是在这之前，没有真正实践过MVC模式，对业务逻辑的处理和对数据的操作没有分的太清楚。后续对代码还进行了大规模重构。
3.	第三个是技术上，这个项目里面用了JPA和JSF的框架，这是我在之前没有接触过的，然后就是现学现用的。



# 5. 酒迅达
这个项目是老师外面接过来的项目，甲方是昆明的一家销售酒水的公司，他们想要一个线上销售平台，像鲜丰水果一样，当天内可以送达的。然后他们就在昆明市的各个区会开设点，根据送达位置选择由哪个点发货。

我们组加上我一共有三个人，然后我在里面担任组长，这也是我第一次担任组长，和别人协作

既然是和别人协作，我就要对组员负责。我要把握好整个项目的节奏，要分配好每个组员的任务。一开始我们是共同讨论，确定项目应该有什么功能。我根据讨论的结果设计好类图，建好数据库，然后搭建了系统的框架。在这里面最大的困难是我的组员和之前的我一样，没有过项目经历，所以我写了一个简单demo, 大概和他们介绍了下项目的结构，向他们解释什么是service,什么是dao。然后把任务分配给他们，提供他们框架的学习资料，让他们边学边做。

我觉得在这里面我做的比较好的是，我没有非常逼迫他们，没有觉得他们都是新手，没有觉得教他们还不如我自己做，我还是很耐心的告诉他们我所知道的，让他们在这个当中能有所收获。因为我觉得作为一名程序员，我不应该只是懂得自己写代码，这是最基础的，我应该学会和别人合作。包括在阿里实习的这几个月，我觉得写代码的时间不是最多的，和别人沟通也是很重要的一部分。

1.	团队协作的问题，组员没有经验，基础不好
集中起来，大概说一下整个项目框架，给他们讲讲每个层次的关联，每个层次都是写什么。就比如backbean层是调用service层的接口，他不存放具体逻辑的，他给页面提供数据。
半个月的时间熟悉一下项目的技术框架。
2.	分工问题：设计好数据库，搭建好系统框架。购物车、收藏、浏览记录、卖方的订单管理。按照能力分、更愿意做的








我想从营销平台是干什么的、我在营销平台干了什么两个角度来讲一下。



# 营销平台是干什么的：
从最朴素的商品组织和售卖过程讲起，商家只要在商品中心填写商品信息、发布商品，消费者就能通过淘宝的搜索就能搜到对应的商品并购买。但是在淘宝天猫的体量
变得如此大，电商业务的竞争也越来越激烈的情况下，仅仅通过消费者主动搜索是很难满足需求的，也很难在大量的物品中找到自己心仪的那一个。所以营销就是帮助
商家更好的将商品卖出去，也让消费者能更快的触达到优质的心仪商品。例如双11、618这些活动是营销，聚划算、特价版、天天特卖也是营销。

之所以是营销平台，是因为我们支撑着阿里电商业务的营销，提供基础和通用的营销能力。我们面向的用户对内是运营，对外是商家。对运营来说，我们提供制定营销方案
的能力，指定出的方案称为营销活动，也是一个核心模型。营销活动一个营销活动定义了一个业务场景详细的营销需求，如需要展示给消费者什么信息，什么样的商家/商品能参加，优惠是普通优惠还是定向优惠、渠道优惠，库存怎么控制等。
对商家来说，我们提供参与营销活动的能力。每个商家可以选择自己意向的活动，然后根据活动的要求提报商品，产出的模型是营销方案。每个商家参加某个活动都会有一份营销方案的数据。
商家报名商品后，经过运营侧的审核和挑选，最终会将商品通过手淘传递给消费者。


# 我在营销平台干了什么 
整个营销平台有35人，分别是招商、选品、投放三个大模块，我所在的团队是招商团队，负责对运营的活动组织和对商家的商家营销，我就属于商家营销领域




# 商品招商策略
## 背景
运营在创建活动的时候会绑定商品池，不同的商品池会有不同的业务目的。例如绑定卡报名的商品池，商家报名活动的时候只能选择池子里的商品报名，或者是绑定优质商品池，在商家报名活动的时候鼓励他报名这个池子里的优质商品。

原来我们实现的方式有很多问题：
1. 扩展性问题，产品提出一个新类型的池子，就要对这个类型做新的开发，包括数据链路、商家报名链路，扩展性很差。如果想绑在其他维度，不支持
2. 性能问题，商品池和商品之间是一对多的关系，商品池和活动也是一对多的关系，但是原来的存储是把这些关系拉平了，会出现一个商品池如果被多个活动绑定，那么每个活动针对这个池子的绑定都会有一条活动ID到商品ID的记录，那这个数据量是爆炸式增长
3. 迭代性，原来对多个类型的池子的实现方式是不统一的，那可维护性和和迭代性都非常查。

那其实不管活动上会绑定什么商品池，他们的关系都可以抽象成活动+商品池+池子类型，基于这个关系，将所有活动上绑定的池子都统一起来。


## 设计、技术选型
可用性、性能、一致性，为什么这么设计？好处


具体方案设计分为两部分：
1. 数据链路，数据怎么产出给商家端用

商家报名商品的方式是在一个商品选择器中选择，要支持搜索的功能。这里我们选择了阿里的OpenSearch搜索引擎，我们的问题归结于怎么把数据灌输到OpenSearch


对数据我们从两方面来考虑，一是可靠性，二是实时性。
为了保证数据可靠正确，


2. 商家端在线链路，怎么使用绑定的商品池


数据最终的展现形式是商家在报名的时候能看到一个商品选择器，可以在里面搜索商品进行报名。
先说一下数据模型，因为我们要得到的是领域-商品池-商品的关系，所以我选择建立两张DB表。一张表是。第二张表是商品池、商品的绑定关系，以商品Id作为主键，有一个商品池Id的多值列。下面拿活动ID来讲

因为要支持名称搜索，我选择将数据存在opensearch搜索引擎。数据的形式是：商品ID+活动ID多值列+商品池ID多值列+商家ID，问题转换成如何把数据灌到搜索引擎里面。
我们从两个方面来考虑，一个是数据的可靠性，一个是实时性。

先说可靠性，因为活动 商品池 商品之间的绑定关系随时都会变，但是要保证不变的情况下搜索引擎里的数据是对的。所以首先需要天级别的数据全量回流到搜索引擎中。需要回流的商品数据大约在7000-9000万左右，一般需要回流4-6个小时。回流的数据源使用的是ODPS，他是基于SQL语句的海量数据平台，对标的是hive。

首先建立一张领域和商品池绑定关系，里面包含领域ID、领域类型、商品池ID、商品池类型，使用领域ID是为了支持活动以外的绑定。每次运营保存活动的时候就将这个关系保存进DB，然后将DB回流到ODPS，过滤掉已经结束的、无效的活动。然后和选品团队的商品-商品池ODPS根据商品池ID做join得到结果表，将结果表每天一次回流到OpenSearch。


然后是实时性，因为运营每天都会创建活动，或者更新活动绑定的商品池，如果每天都要等一天商家才能看到的话太慢了。所以业务上要求说要达到小时级别的更新，但如果每小时都将几千万数据回流一遍压力太大了，而且小时内也回流不完。所以采取增量更新的方式，那就要计算出这个小时和上一个小时比哪些商品发生了变化。
没小时对结果表和上一个小时的数据作对比，并用version字段来标识。如果在上一个小时里没有这个商品，说明是新加的，version字段置为当前时间戳。如果上一个小时有，这一个小时没有，说明这个商品应该删除，version字段置为0，如果两个小时都有，但是商品池ID不一样，那就更新为最新一小时的商品池ID。因为通过ODPS更新OpenSearch不支持部分增量更新，只支持覆盖式更新，所以对于这些diff数据只能通过在线链路更新到OpenSearch。
当diff数据算完之后，通过shell脚本，直接curl直接出发一个http接口，通过应用读取diff数据，对每条数据都给集群发送一个metaQ消息，应用集群就会去消费消息，根据version字段做对应的插入和删除动作。这里会有一个问题是如果数据量很大，那么metaQ消息发送量也会很大，集群消费可能会有压力。所以做了一个限流机制，如果每秒数据发送量大于400，就会让发送消息的线程休息1s再发。


## 分工


## 具体实现


## 难点
对原来链路的梳理和修改非常困难。

## 效果
在这次618需求中，有一个需求就是添加新类型的池子，改动成本非常的小，只是加了一个枚举类，后面的链路就天然支持了



# 规则白盒化
商家是不是能报某个活动,或者是否能报哪个商品会校验很多规则，比如对报名时间、商家身份等的校验。在之前整个校验模块完全是一个黑盒，商家、产品都不知道商家会过什么规则，只有开发通过代码看目前系统里面有什么规则。然后集团今年的一个大目标就是商家体验这一块，所以我们想做到所见即所得，系统里过了什么规则都将他产品化的表达出来。

规则校验分为两种：
Fast Fail快速失败，当某条规则校验不通过时，则抛出错误，或返回失败原因。
All Execute穿透执行，校验所有规则，返回失败原因或返回校验结果

原来的规则实现是这样的：有一个Rule的父类，他有一个check方法，所有的规则都实现check方法，并且把Rule定义在规则枚举类中，在校验的时候就获取所有的Rule进行校验。规则枚举类分为两个维度，一个是业务模型，一个是业务操作。例如商品模型下面有报名、编辑、删除操作，所以确定要执行什么规则就要确定这两个维度。这样有几个问题：
1. 如果要修改rule返回的文案，就要拉分支改代码发布
2. 很多校验逻辑写到了web应用中
3. 某一个规则中堆积了很多代码
4. 后端维护成本高，当商家报名失败来咨询时，只能由开发去翻代码看是被哪个规则卡主了
5. 每个业务需要校验的规则不同，比如聚划算业务可能不校验A B两个规则，那就要在这两个规则中分别写一个判断来跳过聚划算业务
6. 因为我们做的是平台，有很多业务都会有自己的定制规则，那以前都是我们接需求开发，但现在希望他们自己进来开发

所以我们需要把规则抽象成一个模型，然后通过平台方式录入这个规则。规则模型里面包含ID、唯一的code、规则名称、规则描述、所属的领域模型

还有一个规则配置模型，他将什么情况下执行什么规则抽象出来了。包含的字段有执行表达式、规则code列表、操作类型ID

整个分为两部分：
1. 平台能力建设
包含对valve的录入、编辑、查看、对表达式的配置，在创建一个valve的时候需要选择它对应的业务模型和挂载到那个操作下面。创建完valve后会返回一个模板类，开发只要在这个模板基础上实现逻辑。将枚举类中的配置关系可视化到界面上，对开发和产品来说都更好维护和理解。外部开发不需要知道整个底层是怎么运行的， 也不需要去代码里看这个规则应该配置在哪里，只需要在界面上选择可以。对产品来说，他可以更清楚校验了什么规则。


并且对规则是否需要执行，采用了可配置化的方式，在界面上可以直接配置条件，配置当满足条件时应该实行哪些valve。

后台执行的逻辑时，首先根据业务模型和业务操作获取会组合成一个operationId，根据这个ID获取到他下面的所有规则，然后执行mapping rule过滤不符合执行条件的规则。


2. 规则迁移
将老体系的rule迁移到新体系的valve，大约有100多个。对逻辑的梳理，对代码的重构、优化。比如某个规则对不同业务有不同的处理方式，原来的代码里就是if else的去判断。那就可以对这个规则定义一个父类，然后每个业务都实现这个父类，定义自己的逻辑。然后定义一个路由规则，根据不同的业务路由到不同的规则。

因为这相当于将应用的核心代码做了一个翻新，所有不能一下子切到新模式去，需要一个灰度策略慢慢的去切，等线上稳定之后再废弃新模式。我么灰度的粒度是到每一个规则级别。
所以我们的灰度策略是：定义一份配置，里面是原来的rule和新的valve之间的映射关系，还有一个字段标识这个rule是否需要切到新模式执行,原来的rule可能拆分成多个valve，也可能多个rule合并成一个valve，他们之间是多对多的关系。但是我们只建立单项的rule到valve之间的关系，也就是说一个valve可能配置在多个rule下面。因为有一个大前提是规则可以重复执行，但是不能少执行。
所以就读取所有需要新链路执行的valve走新链路，剩余的就继续走老老链路。


# 商家端无线化

因为集团今年的目标时提升商家体验，并且想把所有的商家流量都收口到千牛上。在之前商家只能在PC端的商家营销中心操作，获取营销信息成本高。所以我们希望能做一个移动版的商家营销中心，以小程序方式嵌入到千牛，给商家提供移动端的报名查看和操作。

这个项目分为两期，第一期是活动信息的查看，第二期是活动的报名，我目前参与了第一期。

第一期我们主要是想把关键的信息展现给商家，那站在一个商家的角度上来说。我最关系的数据有这么几个方面：
1. 现在有什么活动
2. 在这么多活动中我能参加什么活动
3. 我曾经参加了什么活动
4. 在这么活动中我最关心的活动是哪些


和PC端相比，主要做了2个维度的优化：
1. 活动组织形式：因为不同业务，活动组织的形式是不一样的，对于很多商家来说理解成本非常高。所以和PC端相比，我们简化了活动结构，定义了前台活动的概念将复杂的活动组织扁平化。对商家来说，看到的都是前台活动，每个前台活动都关联这个一个后台活动。
2. 活动发现形式：原来在PC端，活动非常多，入口非常多，商家理解和操作起来非常困难。没有很清晰的结构来向商家展示能报啥、报了啥，也不能像购物一样收藏我感兴趣的活动。所以我们给商家展示了四个tab的数据，分别是全部活动，可报活动，已报活动，我的收藏。
其次给前台活动加了活动特色的概念，运营在创建活动的时候用几个关键字定义这个活动的卖点是什么，然后商家可以根据特色快速筛选出自己想要的类别。

那整个项目分为两大块：
1. 前台活动数据和可报数据的计算和产出
要建立后台活动和前台活动的映射关系，并且回流到DB和OpenSearch

2. 在线链路的开发
我主要负责在线链路的开发。
对后端来说，就是怎么产出前台活动的信息，主要分成两个步骤。第一个步骤是召回，我们从前台活动表里召回基本数据，不可能所有数据都存在一张表里。例如前台活动名称、报名时间等，其余的很多例如活动特色都是需要后期补全的。因为前台活动是对后台活动组织的统一，所以需要补全的信息还是要从具体的后台模型中补全。
这里的难点在于怎么样优雅的设计能让各后台活动的补全变得清楚，






幂等性怎么确保的？
事务
缓存
DB优化
创建订单的时候存到Redis的是什么？有没有解决过缓存穿透的问题？


